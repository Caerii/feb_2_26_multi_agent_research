<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/" xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns="http://www.w3.org/2005/Atom">
  <id>https://arxiv.org/api/oGYX8cgnbHs4oI8imUROhKQrYzI</id>
  <title>arXiv Query: search_query=all:"multi-agent benchmark"&amp;id_list=&amp;start=0&amp;max_results=20</title>
  <updated>2026-02-07T04:58:40Z</updated>
  <link href="https://arxiv.org/api/query?search_query=all:%22multi-agent+benchmark%22&amp;start=0&amp;max_results=20&amp;id_list=" type="application/atom+xml"/>
  <opensearch:itemsPerPage>20</opensearch:itemsPerPage>
  <opensearch:totalResults>34</opensearch:totalResults>
  <opensearch:startIndex>0</opensearch:startIndex>
  <entry>
    <id>http://arxiv.org/abs/2410.07553v5</id>
    <title>COMMA: A Communicative Multimodal Multi-Agent Benchmark</title>
    <updated>2025-12-16T18:36:40Z</updated>
    <link href="https://arxiv.org/abs/2410.07553v5" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2410.07553v5" rel="related" type="application/pdf" title="pdf"/>
    <summary>The rapid advances of multimodal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce COMMA: a novel puzzle benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark features a variety of multimodal puzzles, providing a comprehensive evaluation across four key categories of agentic capability in a communicative collaboration setting. Our findings reveal surprising weaknesses in state-of-the-art models, including strong proprietary models like GPT-4o and reasoning models like o4-mini. Many chain of thought reasoning models such as R1-Onevision and LLaVA-CoT struggle to outperform even a random baseline in agent-agent collaboration, indicating a potential growth area in their communication abilities.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-10-10T02:49:47Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <arxiv:journal_ref>Transactions on Machine Learning Research, 2025</arxiv:journal_ref>
    <author>
      <name>Timothy Ossowski</name>
    </author>
    <author>
      <name>Danyal Maqbool</name>
    </author>
    <author>
      <name>Jixuan Chen</name>
    </author>
    <author>
      <name>Zefan Cai</name>
    </author>
    <author>
      <name>Tyler Bradshaw</name>
    </author>
    <author>
      <name>Junjie Hu</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.07124v2</id>
    <title>Pommerman: A Multi-Agent Playground</title>
    <updated>2022-04-21T13:52:02Z</updated>
    <link href="https://arxiv.org/abs/1809.07124v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1809.07124v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>We present Pommerman, a multi-agent environment based on the classic console game Bomberman. Pommerman consists of a set of scenarios, each having at least four players and containing both cooperative and competitive aspects. We believe that success in Pommerman will require a diverse set of tools and methods, including planning, opponent/teammate modeling, game theory, and communication, and consequently can serve well as a multi-agent benchmark. To date, we have already hosted one competition, and our next one will be featured in the NIPS 2018 competition track.</summary>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2018-09-19T11:27:25Z</published>
    <arxiv:comment>Oral at the AIIDE Multi-Agent Workshop; 0xc8Ac61A4025B35e425b829fCFCab37f038993963</arxiv:comment>
    <arxiv:primary_category term="cs.MA"/>
    <author>
      <name>Cinjon Resnick</name>
    </author>
    <author>
      <name>Wes Eldridge</name>
    </author>
    <author>
      <name>David Ha</name>
    </author>
    <author>
      <name>Denny Britz</name>
    </author>
    <author>
      <name>Jakob Foerster</name>
    </author>
    <author>
      <name>Julian Togelius</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.00875v2</id>
    <title>TransLaw: A Large-Scale Dataset and Multi-Agent Benchmark Simulating Professional Translation of Hong Kong Case Law</title>
    <updated>2026-01-29T11:39:17Z</updated>
    <link href="https://arxiv.org/abs/2507.00875v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2507.00875v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Hong Kong case law translation presents significant challenges: manual methods suffer from high costs and inconsistent quality, while both traditional machine translation and approaches relying solely on Large Language Models (LLMs) often fail to ensure legal terminology accuracy, culturally embedded nuances, and strict linguistic structures. To overcome these limitations, this study proposes TransLaw, a multi-agent framework that decomposes translation into word-level expression, sentence-level translation, and multidimensional review, integrating a specialized Hong Kong legal glossary database, Retrieval-Augmented Generation (RAG), and iterative feedback. Experiments on our newly constructed HKCFA Judgment 97-22 dataset, benchmarking 13 open-source and commercial LLMs, demonstrate that TransLaw significantly outperforms single-agent baselines across all evaluated models. Human evaluation confirms the framework's effectiveness in terms of legal semantic accuracy, structural coherence, and stylistic fidelity, while noting that it still trails human experts in contextualizing complex terminology and stylistic naturalness.</summary>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-07-01T15:39:26Z</published>
    <arxiv:comment>Original: arXiv:2501.09444; Revised: arXiv:2507.00875; Submitted to ACL 2026</arxiv:comment>
    <arxiv:primary_category term="cs.CL"/>
    <author>
      <name>Xi Xuan</name>
    </author>
    <author>
      <name>Chunyu Kit</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.11703v1</id>
    <title>Action Guidance with MCTS for Deep Reinforcement Learning</title>
    <updated>2019-07-25T19:19:42Z</updated>
    <link href="https://arxiv.org/abs/1907.11703v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1907.11703v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Deep reinforcement learning has achieved great successes in recent years, however, one main challenge is the sample inefficiency. In this paper, we focus on how to use action guidance by means of a non-expert demonstrator to improve sample efficiency in a domain with sparse, delayed, and possibly deceptive rewards: the recently-proposed multi-agent benchmark of Pommerman. We propose a new framework where even a non-expert simulated demonstrator, e.g., planning algorithms such as Monte Carlo tree search with a small number rollouts, can be integrated within asynchronous distributed deep reinforcement learning methods. Compared to a vanilla deep RL algorithm, our proposed methods both learn faster and converge to better policies on a two-player mini version of the Pommerman game.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2019-07-25T19:19:42Z</published>
    <arxiv:comment>AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE'19). arXiv admin note: substantial text overlap with arXiv:1904.05759, arXiv:1812.00045</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Bilal Kartal</name>
    </author>
    <author>
      <name>Pablo Hernandez-Leal</name>
    </author>
    <author>
      <name>Matthew E. Taylor</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.09533v1</id>
    <title>Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?</title>
    <updated>2020-11-18T20:29:59Z</updated>
    <link href="https://arxiv.org/abs/2011.09533v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2011.09533v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Most recently developed approaches to cooperative multi-agent reinforcement learning in the \emph{centralized training with decentralized execution} setting involve estimating a centralized, joint value function. In this paper, we demonstrate that, despite its various theoretical shortcomings, Independent PPO (IPPO), a form of independent learning in which each agent simply estimates its local value function, can perform just as well as or better than state-of-the-art joint learning approaches on popular multi-agent benchmark suite SMAC with little hyperparameter tuning. We also compare IPPO to several variants; the results suggest that IPPO's strong performance may be due to its robustness to some forms of environment non-stationarity.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-11-18T20:29:59Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Christian Schroeder de Witt</name>
    </author>
    <author>
      <name>Tarun Gupta</name>
    </author>
    <author>
      <name>Denys Makoviichuk</name>
    </author>
    <author>
      <name>Viktor Makoviychuk</name>
    </author>
    <author>
      <name>Philip H. S. Torr</name>
    </author>
    <author>
      <name>Mingfei Sun</name>
    </author>
    <author>
      <name>Shimon Whiteson</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2507.21631v1</id>
    <title>"Teammates, Am I Clear?": Analysing Legible Behaviours in Teams</title>
    <updated>2025-07-29T09:40:18Z</updated>
    <link href="https://arxiv.org/abs/2507.21631v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2507.21631v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In this paper we investigate the notion of legibility in sequential decision-making in the context of teams and teamwork. There have been works that extend the notion of legibility to sequential decision making, for deterministic and for stochastic scenarios. However, these works focus on one agent interacting with one human, foregoing the benefits of having legible decision making in teams of agents or in team configurations with humans. In this work we propose an extension of legible decision-making to multi-agent settings that improves the performance of agents working in collaboration. We showcase the performance of legible decision making in team scenarios using our proposed extension in multi-agent benchmark scenarios. We show that a team with a legible agent is able to outperform a team composed solely of agents with standard optimal behaviour.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-07-29T09:40:18Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Miguel Faria</name>
    </author>
    <author>
      <name>Francisco S. Melo</name>
    </author>
    <author>
      <name>Ana Paiva</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.11788v1</id>
    <title>On Hard Exploration for Reinforcement Learning: a Case Study in Pommerman</title>
    <updated>2019-07-26T20:36:09Z</updated>
    <link href="https://arxiv.org/abs/1907.11788v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1907.11788v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>How to best explore in domains with sparse, delayed, and deceptive rewards is an important open problem for reinforcement learning (RL). This paper considers one such domain, the recently-proposed multi-agent benchmark of Pommerman. This domain is very challenging for RL --- past work has shown that model-free RL algorithms fail to achieve significant learning without artificially reducing the environment's complexity. In this paper, we illuminate reasons behind this failure by providing a thorough analysis on the hardness of random exploration in Pommerman. While model-free random exploration is typically futile, we develop a model-based automatic reasoning module that can be used for safer exploration by pruning actions that will surely lead the agent to death. We empirically demonstrate that this module can significantly improve learning.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2019-07-26T20:36:09Z</published>
    <arxiv:comment>AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE) 2019</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Chao Gao</name>
    </author>
    <author>
      <name>Bilal Kartal</name>
    </author>
    <author>
      <name>Pablo Hernandez-Leal</name>
    </author>
    <author>
      <name>Matthew E. Taylor</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2511.03348v2</id>
    <title>Learning Communication Skills in Multi-task Multi-agent Deep Reinforcement Learning</title>
    <updated>2025-11-06T15:16:18Z</updated>
    <link href="https://arxiv.org/abs/2511.03348v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2511.03348v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>In multi-agent deep reinforcement learning (MADRL), agents can communicate with one another to perform a task in a coordinated manner. When multiple tasks are involved, agents can also leverage knowledge from one task to improve learning in other tasks. In this paper, we propose Multi-task Communication Skills (MCS), a MADRL with communication method that learns and performs multiple tasks simultaneously, with agents interacting through learnable communication protocols. MCS employs a Transformer encoder to encode task-specific observations into a shared message space, capturing shared communication skills among agents. To enhance coordination among agents, we introduce a prediction network that correlates messages with the actions of sender agents in each task. We adapt three multi-agent benchmark environments to multi-task settings, where the number of agents as well as the observation and action spaces vary across tasks. Experimental results demonstrate that MCS achieves better performance than multi-task MADRL baselines without communication, as well as single-task MADRL baselines with and without communication.</summary>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-11-05T10:34:44Z</published>
    <arxiv:comment>20 pages, 10 figures</arxiv:comment>
    <arxiv:primary_category term="cs.MA"/>
    <author>
      <name>Changxi Zhu</name>
    </author>
    <author>
      <name>Mehdi Dastani</name>
    </author>
    <author>
      <name>Shihan Wang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02128v2</id>
    <title>No Press Diplomacy: Modeling Multi-Agent Gameplay</title>
    <updated>2019-11-19T19:32:29Z</updated>
    <link href="https://arxiv.org/abs/1909.02128v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1909.02128v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Diplomacy is a seven-player non-stochastic, non-cooperative game, where agents acquire resources through a mix of teamwork and betrayal. Reliance on trust and coordination makes Diplomacy the first non-cooperative multi-agent benchmark for complex sequential social dilemmas in a rich environment. In this work, we focus on training an agent that learns to play the No Press version of Diplomacy where there is no dedicated communication channel between players. We present DipNet, a neural-network-based policy model for No Press Diplomacy. The model was trained on a new dataset of more than 150,000 human games. Our model is trained by supervised learning (SL) from expert trajectories, which is then used to initialize a reinforcement learning (RL) agent trained through self-play. Both the SL and RL agents demonstrate state-of-the-art No Press performance by beating popular rule-based bots.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2019-09-04T21:48:04Z</published>
    <arxiv:comment>Accepted at NeurIPS 2019</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Philip Paquette</name>
    </author>
    <author>
      <name>Yuchen Lu</name>
    </author>
    <author>
      <name>Steven Bocco</name>
    </author>
    <author>
      <name>Max O. Smith</name>
    </author>
    <author>
      <name>Satya Ortiz-Gagne</name>
    </author>
    <author>
      <name>Jonathan K. Kummerfeld</name>
    </author>
    <author>
      <name>Satinder Singh</name>
    </author>
    <author>
      <name>Joelle Pineau</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05759v1</id>
    <title>Safer Deep RL with Shallow MCTS: A Case Study in Pommerman</title>
    <updated>2019-04-10T14:34:40Z</updated>
    <link href="https://arxiv.org/abs/1904.05759v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/1904.05759v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Safe reinforcement learning has many variants and it is still an open research problem. Here, we focus on how to use action guidance by means of a non-expert demonstrator to avoid catastrophic events in a domain with sparse, delayed, and deceptive rewards: the recently-proposed multi-agent benchmark of Pommerman. This domain is very challenging for reinforcement learning (RL) --- past work has shown that model-free RL algorithms fail to achieve significant learning. In this paper, we shed light into the reasons behind this failure by exemplifying and analyzing the high rate of catastrophic events (i.e., suicides) that happen under random exploration in this domain. While model-free random exploration is typically futile, we propose a new framework where even a non-expert simulated demonstrator, e.g., planning algorithms such as Monte Carlo tree search with small number of rollouts, can be integrated to asynchronous distributed deep reinforcement learning methods. Compared to vanilla deep RL algorithms, our proposed methods both learn faster and converge to better policies on a two-player mini version of the Pommerman game.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2019-04-10T14:34:40Z</published>
    <arxiv:comment>Adaptive Learning Agents (ALA) Workshop at AAMAS 2019. arXiv admin note: substantial text overlap with arXiv:1812.00045</arxiv:comment>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Bilal Kartal</name>
    </author>
    <author>
      <name>Pablo Hernandez-Leal</name>
    </author>
    <author>
      <name>Chao Gao</name>
    </author>
    <author>
      <name>Matthew E. Taylor</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.05255v1</id>
    <title>TeamCraft: A Benchmark for Multi-Modal Multi-Agent Systems in Minecraft</title>
    <updated>2024-12-06T18:41:16Z</updated>
    <link href="https://arxiv.org/abs/2412.05255v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2412.05255v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Collaboration is a cornerstone of society. In the real world, human teammates make use of multi-sensory data to tackle challenging tasks in ever-changing environments. It is essential for embodied agents collaborating in visually-rich environments replete with dynamic interactions to understand multi-modal observations and task specifications. To evaluate the performance of generalizable multi-modal collaborative agents, we present TeamCraft, a multi-modal multi-agent benchmark built on top of the open-world video game Minecraft. The benchmark features 55,000 task variants specified by multi-modal prompts, procedurally-generated expert demonstrations for imitation learning, and carefully designed protocols to evaluate model generalization capabilities. We also perform extensive analyses to better understand the limitations and strengths of existing approaches. Our results indicate that existing models continue to face significant challenges in generalizing to novel goals, scenes, and unseen numbers of agents. These findings underscore the need for further research in this area. The TeamCraft platform and dataset are publicly available at https://github.com/teamcraft-bench/teamcraft.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-12-06T18:41:16Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Qian Long</name>
    </author>
    <author>
      <name>Zhi Li</name>
    </author>
    <author>
      <name>Ran Gong</name>
    </author>
    <author>
      <name>Ying Nian Wu</name>
    </author>
    <author>
      <name>Demetri Terzopoulos</name>
    </author>
    <author>
      <name>Xiaofeng Gao</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.11761v2</id>
    <title>Harnessing Language for Coordination: A Framework and Benchmark for LLM-Driven Multi-Agent Control</title>
    <updated>2025-04-22T11:24:23Z</updated>
    <link href="https://arxiv.org/abs/2412.11761v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2412.11761v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. Their potential to facilitate human coordination with many agents is a promising but largely under-explored area. Such capabilities would be helpful in disaster response, urban planning, and real-time strategy scenarios. In this work, we introduce (1) a real-time strategy game benchmark designed to evaluate these abilities and (2) a novel framework we term HIVE. HIVE empowers a single human to coordinate swarms of up to 2,000 agents through a natural language dialog with an LLM. We present promising results on this multi-agent benchmark, with our hybrid approach solving tasks such as coordinating agent movements, exploiting unit weaknesses, leveraging human annotations, and understanding terrain and strategic points. Our findings also highlight critical limitations of current models, including difficulties in processing spatial visual information and challenges in formulating long-term strategic plans. This work sheds light on the potential and limitations of LLMs in human-swarm coordination, paving the way for future research in this area. The HIVE project page, hive.syrkis.com, includes videos of the system in action.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2024-12-16T13:25:42Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Timothée Anne</name>
    </author>
    <author>
      <name>Noah Syrkis</name>
    </author>
    <author>
      <name>Meriem Elhosni</name>
    </author>
    <author>
      <name>Florian Turati</name>
    </author>
    <author>
      <name>Franck Legendre</name>
    </author>
    <author>
      <name>Alain Jaquier</name>
    </author>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <arxiv:doi>10.1109/TG.2025.3564042</arxiv:doi>
    <link rel="related" href="https://doi.org/10.1109/TG.2025.3564042" title="doi"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.03722v1</id>
    <title>On the Robustness of Cooperative Multi-Agent Reinforcement Learning</title>
    <updated>2020-03-08T05:12:13Z</updated>
    <link href="https://arxiv.org/abs/2003.03722v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2003.03722v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>In cooperative multi-agent reinforcement learning (c-MARL), agents learn to cooperatively take actions as a team to maximize a total team reward. We analyze the robustness of c-MARL to adversaries capable of attacking one of the agents on a team. Through the ability to manipulate this agent's observations, the adversary seeks to decrease the total team reward.
  Attacking c-MARL is challenging for three reasons: first, it is difficult to estimate team rewards or how they are impacted by an agent mispredicting; second, models are non-differentiable; and third, the feature space is low-dimensional. Thus, we introduce a novel attack. The attacker first trains a policy network with reinforcement learning to find a wrong action it should encourage the victim agent to take. Then, the adversary uses targeted adversarial examples to force the victim to take this action.
  Our results on the StartCraft II multi-agent benchmark demonstrate that c-MARL teams are highly vulnerable to perturbations applied to one of their agent's observations. By attacking a single agent, our attack method has highly negative impact on the overall team reward, reducing it from 20 to 9.4. This results in the team's winning rate to go down from 98.9% to 0%.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-03-08T05:12:13Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Jieyu Lin</name>
    </author>
    <author>
      <name>Kristina Dzeparoska</name>
    </author>
    <author>
      <name>Sai Qian Zhang</name>
    </author>
    <author>
      <name>Alberto Leon-Garcia</name>
    </author>
    <author>
      <name>Nicolas Papernot</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.11940v3</id>
    <title>Decision-making with Speculative Opponent Models</title>
    <updated>2024-03-22T04:40:11Z</updated>
    <link href="https://arxiv.org/abs/2211.11940v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2211.11940v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>Opponent modelling has proven effective in enhancing the decision-making of the controlled agent by constructing models of opponent agents. However, existing methods often rely on access to the observations and actions of opponents, a requirement that is infeasible when such information is either unobservable or challenging to obtain. To address this issue, we introduce Distributional Opponent-aided Multi-agent Actor-Critic (DOMAC), the first speculative opponent modelling algorithm that relies solely on local information (i.e., the controlled agent's observations, actions, and rewards). Specifically, the actor maintains a speculated belief about the opponents using the tailored speculative opponent models that predict the opponents' actions using only local information. Moreover, DOMAC features distributional critic models that estimate the return distribution of the actor's policy, yielding a more fine-grained assessment of the actor's quality. This thus more effectively guides the training of the speculative opponent models that the actor depends upon. Furthermore, we formally derive a policy gradient theorem with the proposed opponent models. Extensive experiments under eight different challenging multi-agent benchmark tasks within the MPE, Pommerman and StarCraft Multiagent Challenge (SMAC) demonstrate that our DOMAC successfully models opponents' behaviours and delivers superior performance against state-of-the-art methods with a faster convergence speed.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2022-11-22T01:29:47Z</published>
    <arxiv:comment>13 pages, 27 figures</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Jing Sun</name>
    </author>
    <author>
      <name>Shuo Chen</name>
    </author>
    <author>
      <name>Cong Zhang</name>
    </author>
    <author>
      <name>Yining Ma</name>
    </author>
    <author>
      <name>Jie Zhang</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.07542v1</id>
    <title>A Versatile Multi-Agent Reinforcement Learning Benchmark for Inventory Management</title>
    <updated>2023-06-13T05:22:30Z</updated>
    <link href="https://arxiv.org/abs/2306.07542v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2306.07542v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>Multi-agent reinforcement learning (MARL) models multiple agents that interact and learn within a shared environment. This paradigm is applicable to various industrial scenarios such as autonomous driving, quantitative trading, and inventory management. However, applying MARL to these real-world scenarios is impeded by many challenges such as scaling up, complex agent interactions, and non-stationary dynamics. To incentivize the research of MARL on these challenges, we develop MABIM (Multi-Agent Benchmark for Inventory Management) which is a multi-echelon, multi-commodity inventory management simulator that can generate versatile tasks with these different challenging properties. Based on MABIM, we evaluate the performance of classic operations research (OR) methods and popular MARL algorithms on these challenging tasks to highlight their weaknesses and potential.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-06-13T05:22:30Z</published>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Xianliang Yang</name>
    </author>
    <author>
      <name>Zhihao Liu</name>
    </author>
    <author>
      <name>Wei Jiang</name>
    </author>
    <author>
      <name>Chuheng Zhang</name>
    </author>
    <author>
      <name>Li Zhao</name>
    </author>
    <author>
      <name>Lei Song</name>
    </author>
    <author>
      <name>Jiang Bian</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.12306v4</id>
    <title>Value-Decomposition Multi-Agent Actor-Critics</title>
    <updated>2020-12-18T15:16:06Z</updated>
    <link href="https://arxiv.org/abs/2007.12306v4" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2007.12306v4" rel="related" type="application/pdf" title="pdf"/>
    <summary>The exploitation of extra state information has been an active research area in multi-agent reinforcement learning (MARL). QMIX represents the joint action-value using a non-negative function approximator and achieves the best performance, by far, on multi-agent benchmarks, StarCraft II micromanagement tasks. However, our experiments show that, in some cases, QMIX is incompatible with A2C, a training paradigm that promotes algorithm training efficiency. To obtain a reasonable trade-off between training efficiency and algorithm performance, we extend value-decomposition to actor-critics that are compatible with A2C and propose a novel actor-critic framework, value-decomposition actor-critics (VDACs). We evaluate VDACs on the testbed of StarCraft II micromanagement tasks and demonstrate that the proposed framework improves median performance over other actor-critic methods. Furthermore, we use a set of ablation experiments to identify the key factors that contribute to the performance of VDACs.</summary>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <published>2020-07-24T00:50:02Z</published>
    <arxiv:comment>Accepted by 35th AAAI Conference on Artificial Intelligence (AAAI 2021)</arxiv:comment>
    <arxiv:primary_category term="cs.AI"/>
    <author>
      <name>Jianyu Su</name>
    </author>
    <author>
      <name>Stephen Adams</name>
    </author>
    <author>
      <name>Peter A. Beling</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.18586v2</id>
    <title>Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications</title>
    <updated>2025-10-31T04:17:05Z</updated>
    <link href="https://arxiv.org/abs/2510.18586v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2510.18586v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Large Language Models (LLMs) are increasingly deployed in complex multi-agent applications that use external function calls. This workload creates severe performance challenges for the KV Cache: space contention leads to the eviction of critical agents' caches and time underutilization leaves the cache of agents stalled on long-running tool calls idling in GPU memory. We present Tokencake, a KV-Cache-centric serving framework that co-optimizes scheduling and memory management with an agent-aware design. Tokencake's Space Scheduler uses dynamic memory partitioning to shield critical agents from contention, while its Time Scheduler employs a proactive offload and predictive upload mechanism to repurpose GPU memory during function call stalls. Our evaluation on representative multi-agent benchmarks shows that Tokencake can reduce end-to-end latency by over 47.06%, improve effective GPU memory utilization by up to 16.9% compared to vLLM.</summary>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-10-21T12:39:32Z</published>
    <arxiv:primary_category term="cs.DC"/>
    <author>
      <name>Zhuohang Bian</name>
    </author>
    <author>
      <name>Feiyang Wu</name>
    </author>
    <author>
      <name>Teng Ma</name>
    </author>
    <author>
      <name>Youwei Zhuo</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.08901v1</id>
    <title>Welfare Diplomacy: Benchmarking Language Model Cooperation</title>
    <updated>2023-10-13T07:15:32Z</updated>
    <link href="https://arxiv.org/abs/2310.08901v1" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2310.08901v1" rel="related" type="application/pdf" title="pdf"/>
    <summary>The growing capabilities and increasingly widespread deployment of AI systems necessitate robust benchmarks for measuring their cooperative capabilities. Unfortunately, most multi-agent benchmarks are either zero-sum or purely cooperative, providing limited opportunities for such measurements. We introduce a general-sum variant of the zero-sum board game Diplomacy -- called Welfare Diplomacy -- in which players must balance investing in military conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both a clearer assessment of and stronger training incentives for cooperative capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules and implementing them via an open-source Diplomacy engine; (2) constructing baseline agents using zero-shot prompted language models; and (3) conducting experiments where we find that baselines using state-of-the-art models attain high social welfare but are exploitable. Our work aims to promote societal safety by aiding researchers in developing and assessing multi-agent AI systems. Code to evaluate Welfare Diplomacy and reproduce our experiments is available at https://github.com/mukobi/welfare-diplomacy.</summary>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <published>2023-10-13T07:15:32Z</published>
    <arxiv:primary_category term="cs.MA"/>
    <author>
      <name>Gabriel Mukobi</name>
    </author>
    <author>
      <name>Hannah Erlebach</name>
    </author>
    <author>
      <name>Niklas Lauffer</name>
    </author>
    <author>
      <name>Lewis Hammond</name>
    </author>
    <author>
      <name>Alan Chan</name>
    </author>
    <author>
      <name>Jesse Clifton</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.18138v2</id>
    <title>B3C: A Minimalist Approach to Offline Multi-Agent Reinforcement Learning</title>
    <updated>2025-02-02T23:26:42Z</updated>
    <link href="https://arxiv.org/abs/2501.18138v2" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2501.18138v2" rel="related" type="application/pdf" title="pdf"/>
    <summary>Overestimation arising from selecting unseen actions during policy evaluation is a major challenge in offline reinforcement learning (RL). A minimalist approach in the single-agent setting -- adding behavior cloning (BC) regularization to existing online RL algorithms -- has been shown to be effective; however, this approach is understudied in multi-agent settings. In particular, overestimation becomes worse in multi-agent settings due to the presence of multiple actions, resulting in the BC regularization-based approach easily suffering from either over-regularization or critic divergence. To address this, we propose a simple yet effective method, Behavior Cloning regularization with Critic Clipping (B3C), which clips the target critic value in policy evaluation based on the maximum return in the dataset and pushes the limit of the weight on the RL objective over BC regularization, thereby improving performance. Additionally, we leverage existing value factorization techniques, particularly non-linear factorization, which is understudied in offline settings. Integrated with non-linear value factorization, B3C outperforms state-of-the-art algorithms on various offline multi-agent benchmarks.</summary>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-01-30T05:02:33Z</published>
    <arxiv:primary_category term="cs.LG"/>
    <author>
      <name>Woojun Kim</name>
    </author>
    <author>
      <name>Katia Sycara</name>
    </author>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2510.03534v3</id>
    <title>Long-Term Mapping of the Douro River Plume with Multi-Agent Reinforcement Learning</title>
    <updated>2025-11-03T22:46:52Z</updated>
    <link href="https://arxiv.org/abs/2510.03534v3" rel="alternate" type="text/html"/>
    <link href="https://arxiv.org/pdf/2510.03534v3" rel="related" type="application/pdf" title="pdf"/>
    <summary>We study the problem of long-term (multiple days) mapping of a river plume using multiple autonomous underwater vehicles (AUVs), focusing on the Douro river representative use-case. We propose an energy - and communication - efficient multi-agent reinforcement learning approach in which a central coordinator intermittently communicates with the AUVs, collecting measurements and issuing commands. Our approach integrates spatiotemporal Gaussian process regression (GPR) with a multi-head Q-network controller that regulates direction and speed for each AUV. Simulations using the Delft3D ocean model demonstrate that our method consistently outperforms both single- and multi-agent benchmarks, with scaling the number of agents both improving mean squared error (MSE) and operational endurance. In some instances, our algorithm demonstrates that doubling the number of AUVs can more than double endurance while maintaining or improving accuracy, underscoring the benefits of multi-agent coordination. Our learned policies generalize across unseen seasonal regimes over different months and years, demonstrating promise for future developments of data-driven long-term monitoring of dynamic plume environments.</summary>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <published>2025-10-03T22:08:08Z</published>
    <arxiv:primary_category term="cs.MA"/>
    <author>
      <name>Nicolò Dal Fabbro</name>
    </author>
    <author>
      <name>Milad Mesbahi</name>
    </author>
    <author>
      <name>Renato Mendes</name>
    </author>
    <author>
      <name>João Borges de Sousa</name>
    </author>
    <author>
      <name>George J. Pappas</name>
    </author>
  </entry>
</feed>
